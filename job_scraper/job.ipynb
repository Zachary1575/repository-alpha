{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6db18-41dc-46ae-bbba-26f735af91b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import hashlib\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624f215b-8a30-46d0-83ef-3be319eb29af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50aa12c-60c9-4999-9890-3fd9ed7f5803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecae4a5-c28f-44a3-826f-1d909a77ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492ea99-ab64-443e-80a1-98577fa84790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca602352-e9f7-4b91-a616-e13fe48a07db",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.remove()\n",
    "logger.add(\n",
    "    sink=sys.stdout, \n",
    "    format=\"<green>{time}</green> | <level>{level}</level> | <level>{message}</level>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d443a3df-823c-40fd-ab30-a86431da1a8e",
   "metadata": {},
   "source": [
    "## StillHiring.today Table Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d02864-b8f6-4ba2-8826-57d6ece9785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c45c81-a764-44b1-9b1f-80216228d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://airtable.com/appPGrJqA2zH65k5I/shrI8dno1rMGKZM8y/tblKU0jQiyIX182uU?backgroundColor=cyan&viewControls=on\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b74df85-5a88-4e86-b14b-f44ff58a1fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)\n",
    "table_left = None;\n",
    "table_right = None;\n",
    "\n",
    "job_loop = True;\n",
    "job_finished = False;\n",
    "\n",
    "company_dictionary = {\n",
    "    \"Company-Name\": [],\n",
    "    \"Application-Links\": []\n",
    "};\n",
    "\n",
    "companyNameSet = set()\n",
    "companyLinkSet = set()\n",
    "\n",
    "try:\n",
    "    right_scrollbar = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \".antiscroll-scrollbar-vertical\"))\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    driver.quit()\n",
    "\n",
    "if (right_scrollbar):\n",
    "    actions = ActionChains(driver)\n",
    "\n",
    "    while (job_loop):\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        \n",
    "        company_names = soup.find_all('div', class_='cell primary read')\n",
    "        company_links = soup.find_all('a', class_='link-quiet')\n",
    "        hrefs = [link.get('href') for link in company_links]\n",
    "        \n",
    "        for company in company_names:\n",
    "            if (company.text not in companyNameSet):\n",
    "                company_dictionary[\"Company-Name\"].append(company.text)\n",
    "                if (company.text == \"Axuall\"):\n",
    "                    job_finished = True;\n",
    "                companyNameSet.add(company.text)\n",
    "                \n",
    "        for href in hrefs:\n",
    "            if (company.text not in companyLinkSet):\n",
    "                company_dictionary[\"Application-Links\"].append(href)\n",
    "                companyLinkSet.add(href)\n",
    "\n",
    "        if (job_finished):\n",
    "            job_loop = False;\n",
    "            break;\n",
    "            \n",
    "        actions.click_and_hold(right_scrollbar).perform()\n",
    "        scroll_amount = 10\n",
    "        actions.move_by_offset(0, scroll_amount).perform()\n",
    "        time.sleep(3)  # Adjust sleep time to wait for content to load\n",
    "        actions.release().perform()\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e67426-6a2d-478e-9c80-66cd7d88017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(company_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcf74fe-6851-4078-a867-56c6331863ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ca436-2607-45f1-8a84-368c647e71f9",
   "metadata": {},
   "source": [
    "## Getting Job List of Workday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bc39e2-a526-405c-ba81-f1118c08f4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_workday = df[df['Application-Links'].str.contains(\"workday\", case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddee3c4-4ee6-4677-af00-9ab007bf07cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless=new\") # for Chrome >= 109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abea4e9b-549c-4879-9ff6-1fcdb89a7bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefs = {\n",
    "    \"profile.managed_default_content_settings.images\": 2,  # Disable images\n",
    "}\n",
    "chrome_options.add_experimental_option(\"prefs\", prefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a34702-20a1-4c9e-a0c2-2f2053024ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_workday = pd.read_csv(\"./company_urls.txt\", delimiter=' ')\n",
    "df_workday = df_workday.iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b03cbae-2aee-4950-aa49-18e34fb2dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_workday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e207e3d-8896-4329-9d58-a77807ba2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_pages_into_three_parts(total_pages):\n",
    "    part_size = total_pages // 3\n",
    "    remainder = total_pages % 3\n",
    "\n",
    "    parts = []\n",
    "    start_page = 1\n",
    "\n",
    "    for i in range(3):\n",
    "        end_page = start_page + part_size - 1\n",
    "        if remainder > 0:\n",
    "            end_page += 1\n",
    "            remainder -= 1\n",
    "        \n",
    "        parts.append((start_page, end_page))\n",
    "        start_page = end_page + 1\n",
    "\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4502059b-fcb5-42c3-a008-0c419310605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_current_job_time(previous_predictions, completion_times, alpha = 0.5):\n",
    "    \"\"\"\n",
    "    Update the Exponential Moving Average (EMA) based on the latest completion time\n",
    "    and the previous EMA prediction.\n",
    "\n",
    "    Parameters:\n",
    "    - completion_times: list of float or int, the history of completion times, \n",
    "                        with the last entry being the most recent completion time.\n",
    "    - previous_predictions: list of float, the history of previous EMA predictions, \n",
    "                            with the last entry being the most recent EMA.\n",
    "    - alpha: float, the smoothing factor, where 0 < alpha <= 1. Defaults to 0.1.\n",
    "\n",
    "    Returns:\n",
    "    - float: the updated EMA value.\n",
    "    \"\"\"\n",
    "    ema = alpha * completion_times[len(completion_times) - 1] + (1 - alpha) * previous_predictions[len(previous_predictions) - 1]\n",
    "    return ema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71279003-01ab-405e-b0cd-bd7bdcfe2b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_monitoring_task(duration, stop_event):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while (time.time() - start_time < duration and not stop_event.is_set()):\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "    if (time.time() - start_time < duration):\n",
    "        logger.debug(\"Terminating Timing Thread as host process finished early!\")\n",
    "    else:\n",
    "        logger.warning(\"Time limit reached for current job. Signaling processing task to stop.\")\n",
    "        stop_event.set()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ced048-4353-42d6-9cf4-66481d105355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def workday_job_scraper(url, start, finish, duration):\n",
    "    page = start\n",
    "    prev_page = start\n",
    "    stop_event = threading.Event()\n",
    "    current_thread_name = threading.current_thread().name\n",
    "    executor = ThreadPoolExecutor()\n",
    "    \n",
    "    terms = [\n",
    "        \"software\", \"developer\", \"data\", \".Net\", \"C#\", \"C\", \"C++\",\n",
    "        \"full stack\", \"backend\", \"front end\", \"frontend\", \"backend\", \n",
    "        \"back-end\", \"back end\", \"systems\", \"DevOps\", \"site reliability\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize the thread timer\n",
    "    try:\n",
    "        executor.submit(time_monitoring_task, duration, stop_event)\n",
    "    except Exception as e:\n",
    "        print(f\"[{current_thread_name}] Could not spawn time thread! Exiting!\")\n",
    "        print(f\"{e}\")\n",
    "        total_lost_jobs = (((finish - start) + 1) * 20)\n",
    "        return total_lost_jobs\n",
    "\n",
    "    # Then start scraping\n",
    "    try:\n",
    "        retries = 100\n",
    "        hash_set = set()\n",
    "        \n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.set_page_load_timeout(30)\n",
    "        driver.get(url)\n",
    "        time.sleep(10)\n",
    "            \n",
    "        walk_start = 1\n",
    "        while(walk_start != start and not stop_event.is_set()):\n",
    "            walk_start += 1 \n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            \n",
    "            try:\n",
    "                WebDriverWait(driver, 100).until(EC.element_to_be_clickable((By.XPATH, f\"//button[@data-uxi-widget-type='paginationPageButton' and text()='{walk_start}']\")))\n",
    "            except TimeoutException:\n",
    "                print(f\"Could not find the pagination button for page {walk_start} within the specified time.\")\n",
    "                \n",
    "            buttonA = driver.find_element(By.XPATH, f\"//button[@data-uxi-widget-type='paginationPageButton' and text()='{walk_start}']\")\n",
    "            buttonA.click()\n",
    "\n",
    "        if (stop_event.is_set()):\n",
    "            logger.error(\"Thread Timed out during Walk!\")\n",
    "            total_lost_jobs = (((finish - start) + 1) * 20)\n",
    "            driver.quit()\n",
    "            return total_lost_jobs\n",
    "        else:  \n",
    "            logger.debug(f\"[{current_thread_name}] Finished Walk!\")\n",
    "\n",
    "        retry_cnt = 0\n",
    "        total_lost_jobs = 0\n",
    "        while(page <= finish and retry_cnt != 10 and not stop_event.is_set()):\n",
    "            WebDriverWait(driver, retries).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"li\")))\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "            \n",
    "            li_elements = soup.find_all('li')\n",
    "\n",
    "            \"\"\"Core Scraping Logic\"\"\"\n",
    "            skipped_jobs = 0\n",
    "            duplicate_cnt = 0\n",
    "            for li in li_elements:\n",
    "                job_title_raw = li.find('a', {'data-automation-id': 'jobTitle'})\n",
    "                location_raw = li.find('div', {'data-automation-id': 'locations'})\n",
    "                postedTime_raw = li.find('div', {'data-automation-id': 'postedOn'})\n",
    "                jobID_raw = li.find('ul', {'data-automation-id': 'subtitle'})\n",
    "                \n",
    "                if (job_title_raw and location_raw and postedTime_raw and jobID_raw):\n",
    "                    # Job Title\n",
    "                    job_title = job_title_raw.get_text()\n",
    "                    \n",
    "                    # Location\n",
    "                    location = None\n",
    "                    dd_elements_loc = location_raw.find_all('dd')\n",
    "                    for dd in dd_elements_loc:\n",
    "                        location = dd.get_text()\n",
    "    \n",
    "                    # Time\n",
    "                    postedTime = None\n",
    "                    dd_elements_time = postedTime_raw.find_all('dd')\n",
    "                    for dd in dd_elements_time:\n",
    "                        postedTime = dd.get_text()\n",
    "    \n",
    "                    # Job Link\n",
    "                    job_link = (url.split('.com')[0] + '.com' if '.com' in url else url) + job_title_raw['href']\n",
    "\n",
    "                    # Job ID\n",
    "                    jobID = None\n",
    "                    li_elements_id = jobID_raw.find_all('li')\n",
    "                    readableID = \"\"\n",
    "                    for li in li_elements_id:\n",
    "                        readableID += li.get_text()\n",
    "                    jobID = hashlib.sha256(readableID.encode())\n",
    "                        \n",
    "                    term_found = False\n",
    "                    if (jobID not in hash_set):\n",
    "                        for term in terms:\n",
    "                            if term in job_title.lower():\n",
    "                                term_found = True\n",
    "                        if (term_found):\n",
    "                            list_lock_A = threading.Lock()\n",
    "                            with list_lock_A:\n",
    "                                workday_jobs[\"Job_Title\"].append(job_title)\n",
    "                                workday_jobs[\"Job_Location\"].append(location)\n",
    "                                workday_jobs[\"Job_Posted_Time\"].append(postedTime)\n",
    "                                workday_jobs[\"Job_Link\"].append(job_link)\n",
    "                                workday_jobs[\"Job_ID\"].append(jobID)\n",
    "                                workday_jobs[\"Job_Meta\"].append(readableID)\n",
    "                        hash_set.add(jobID)\n",
    "                    else:\n",
    "                        duplicate_cnt += 1\n",
    "                elif ((job_title_raw == None) ^ (location_raw == None) ^ (postedTime_raw == None) ^ (jobID_raw == None)):\n",
    "                    skipped_jobs += 1\n",
    "\n",
    "            if (duplicate_cnt <= 5): # Having some duplicates is fine (if possible), but a whole page is unlikely\n",
    "                total_lost_jobs += skipped_jobs\n",
    "                total_lost_jobs += duplicate_cnt\n",
    "\n",
    "                prev_page = page\n",
    "                page += 1\n",
    "                if (page > finish):\n",
    "                    break\n",
    "                    \n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                WebDriverWait(driver, retries).until(EC.element_to_be_clickable((By.XPATH, f\"//button[@data-uxi-widget-type='paginationPageButton' and text()='{page}']\")))\n",
    "                buttonB = driver.find_element(By.XPATH, f\"//button[@data-uxi-widget-type='paginationPageButton' and text()='{page}']\")\n",
    "                buttonB.click()\n",
    "\n",
    "                \n",
    "                WebDriverWait(driver, retries).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"li\")))\n",
    "                WebDriverWait(driver, retries).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"button\")))\n",
    "            elif (prev_page == page):\n",
    "                retry_cnt += 1\n",
    "                print(f\"[{current_thread_name}] Current Page Contained too many Duplicates! ({duplicate_cnt}) Retrying...\")\n",
    "                time.sleep(3)\n",
    "                \n",
    "        if (retry_cnt == 10):\n",
    "            logger.error(\"Hit Max Retry Count!\")\n",
    "            total_lost_jobs = (((finish - start) + 1) * 20)\n",
    "        elif (stop_event.is_set()):\n",
    "            logger.error(\"Thread Timed out!\")\n",
    "            total_lost_jobs = (((finish - start) + 1) * 20)\n",
    "        \n",
    "        logger.info(f\"[{current_thread_name}] Total Jobs Lost: {total_lost_jobs}\")\n",
    "        driver.quit()\n",
    "        stop_event.set()\n",
    "        executor.shutdown(wait=True)\n",
    "        \n",
    "        return total_lost_jobs\n",
    "    except Exception as e:\n",
    "        current_thread_name = threading.current_thread().name\n",
    "        total_lost_jobs = (((finish - start) + 1) * 20)\n",
    "        print(f\"[ERROR] {current_thread_name} ran into an error!\")\n",
    "        print(f\"[ERROR] on page {page}!\")\n",
    "        logger.exception(f\"Error occcured for {url}\")\n",
    "        return total_lost_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48deb87-de47-47e4-8e7f-c8e623b693c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def workday_job_scraper_multithread(url):\n",
    "    if (len(url.split(\"/\")) >= 5 and \"en-US\" not in url):\n",
    "        logger.info(f\"Skipped link '{url}' due to job workday referring outside the U.S\")\n",
    "        return True\n",
    "        \n",
    "    start_time = time.time()\n",
    "    estimated_time = estimate_current_job_time(historical_EMA_Predictions, actual_completion_times)\n",
    "    ema_with_constant = estimated_time + 300\n",
    "    \n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        driver.set_page_load_timeout(30)\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "    \n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    \n",
    "        driver.quit()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get total number of jobs!\")\n",
    "        print(f\"Error occcured at for {url} : {e}\")\n",
    "        return False\n",
    "    \n",
    "    p_element = soup.find('p', {'data-automation-id': 'jobOutOfText'})\n",
    "    total_jobs = -1\n",
    "    \n",
    "    if (p_element):\n",
    "        total_jobs = int(p_element.get_text().split()[4])\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    total_pages = total_jobs // 20\n",
    "\n",
    "    print(f\"*** FOR {url} ***\")\n",
    "    print(f\"Total number of jobs is: {total_jobs}\")\n",
    "    print(f\"Total number of pages is: {total_pages}\")\n",
    "    print(f\"Estimated time for completion of longest running thread: {estimated_time}\")\n",
    "\n",
    "    if (total_pages < 3):\n",
    "        workday_job_scraper(url, 1, total_pages, ema_with_constant)\n",
    "    else:\n",
    "        partition = divide_pages_into_three_parts(total_pages)\n",
    "    \n",
    "        logger.info(f\"Partitioning pages scheme per thread: {partition}\")\n",
    "        logger.debug(\"Executing Threading!\")\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            jobs_to_scrape = [\n",
    "                (f\"{url}\", partition[0][0], partition[0][1], ema_with_constant),\n",
    "                (f\"{url}\", partition[1][0], partition[1][1], ema_with_constant),\n",
    "                (f\"{url}\", partition[2][0], partition[2][1], ema_with_constant)\n",
    "            ]\n",
    "            futures = [executor.submit(workday_job_scraper, url, start, finish, duration) for url, start, finish, duration in jobs_to_scrape]\n",
    "\n",
    "        overall_jobs_lost = 0\n",
    "        for future in futures:\n",
    "            overall_jobs_lost += future.result()\n",
    "        percentage = (overall_jobs_lost / total_jobs) * 100\n",
    "        job_loss_rate_arr.append(percentage)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        actual_completion_times.append(elapsed_time)\n",
    "        historical_EMA_Predictions.append(estimated_time)\n",
    "        \n",
    "        print(\"\\n --- Scrape Statistics --- \")\n",
    "        print(f\"Execution time: {elapsed_time} seconds\")\n",
    "        print(f\"Jobs Lost this Workday Link: {overall_jobs_lost}\")\n",
    "        print(f\"Jobs Loss: {percentage}%\")\n",
    "        print(f\"Average Job Loss Rate: {math.trunc((sum(job_loss_rate_arr) / len(job_loss_rate_arr)) * 100) / 100}% \\n\\n\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5021bae-c260-4d26-a973-c9f0afaccd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "workday_jobs = {}\n",
    "workday_jobs[\"Job_Title\"] = []\n",
    "workday_jobs[\"Job_Location\"] = []\n",
    "workday_jobs[\"Job_Posted_Time\"] = []\n",
    "workday_jobs[\"Job_Link\"] = []\n",
    "workday_jobs[\"Job_ID\"] = []\n",
    "workday_jobs[\"Job_Meta\"] = []\n",
    "\n",
    "job_loss_rate_arr = []\n",
    "\n",
    "historical_EMA_Predictions = [0] # Initialize with 200 Seconds of EMA\n",
    "actual_completion_times = [200] # Initalize with actual time\n",
    "\n",
    "df_workday[\"Application-Links\"].progress_apply(workday_job_scraper_multithread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a552443-598b-4b02-b6cc-4d633eef3cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_workday_master = pd.DataFrame(workday_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95714ee4-7f2d-4c44-b681-0419e7bc4287",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(jobs_workday_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f871bd15-925b-461a-9fba-12a41e83e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_workday_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad693c-ac6f-48fe-8773-c655565e02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def converTimeToNum(x):\n",
    "    if (x == \"Posted Today\"):\n",
    "        return 0\n",
    "    elif (x == \"Posted Yesterday\"):\n",
    "        return 1\n",
    "    num = x.split()[1]\n",
    "    if (num == \"30+\"):\n",
    "        return 30\n",
    "    \n",
    "    return int(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed91243-e373-4d98-9b17-af3fc8751aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_workday_master[\"Job_Posted_Time\"] = jobs_workday_master[\"Job_Posted_Time\"].progress_apply(converTimeToNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980294fc-d606-426b-b946-9846ea72053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_workday_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d20636-f61a-4efa-831b-ff33a4610ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = jobs_workday_master.sort_values(by='Job_Posted_Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc5263f-f986-4fe9-884d-b90b924e988c",
   "metadata": {},
   "source": [
    "### Now we do a criteria seperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea325aac-d606-4264-8342-d0e99092909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_string(url):\n",
    "    # Find the substring between '//' and the first '/' after it\n",
    "    match = re.search(r'https?://([^./]+)\\.', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb753df0-b3e2-41e9-8471-ff8bca114f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted[\"Company\"] = df_sorted[\"Job_Link\"].progress_apply(extract_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f76601-b7ad-4083-a687-99d4920a680a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc087f-9de3-45f6-bef1-675b87d9e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_strings_titles = [\"Intern\", \"Internship\", \"Temporary\", \"Senior\", \"Lead\", \"Principal\", \"Staff\", \"Manager\", \"Director\", \"Head\", \"Chief\", \"Architect\", \"VP\", \"Vice President\", \"Manager\", \"Sr\"]\n",
    "pattern_titles = '|'.join(exclude_strings_titles)\n",
    "df_filtered = df_sorted[~df_sorted['Job_Title'].str.contains(pattern_titles, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9148d120-bcf8-4ae8-9797-b4cd5c0f9e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_strings_location = [\"Mexico\", \"India\", \"Poland\", \"Toronto\", \"Ireland\", \"Bangalore\", \"China\", \"Pune\", \"Singapore\", \"Bengaluru\", \"Israel\", \"Noida\", \"Manila\", \"Gurgaon\"]\n",
    "pattern_locations = '|'.join(exclude_strings_location)\n",
    "df_filtered = df_filtered[~df_filtered['Job_Location'].str.contains(pattern_locations, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e7e9ad-7791-4f5d-941e-15011e5a1491",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered[df_filtered['Job_Posted_Time'] < 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b456020-f0d4-443c-bef4-077b45bbefff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca3fd47-02f6-42f3-b5d7-1d46a01334c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = df_filtered.drop_duplicates(subset=['Job_Meta'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee399900-1cbb-49c0-b9b9-22da71ba27b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4419c0-a963-4afd-849a-4a2b2bddfc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"./workday_jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2875a7f-976d-4b24-ab13-a4ac47db13c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98e7918-bd00-4d23-b5e3-2f7723fecf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"That's it, beautiful and simple logging!\")\n",
    "logger.debug(\"That's it, beautiful and simple logging!\")\n",
    "logger.warning(\"That's it, beautiful and simple logging!\")\n",
    "logger.error(\"That's it, beautiful and simple logging!\")\n",
    "logger.success(\"That's it, beautiful and simple logging!\")\n",
    "logger.critical(\"That's it, beautiful and simple logging!\")\n",
    "logger.exception(\"That's it, beautiful and simple logging!\", \"ERROR!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68455af-ed53-46d8-bac7-72a467c9e167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74af4cb-6c11-40cb-be34-b88148a8f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dad8d5-8d92-4ebb-b663-65d08005115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"mongodb+srv://zacharygou:jobscrper@cluster0.mv6xv.mongodb.net/\"\n",
    "df = pd.read_csv(\"/Users/zacharyg/Downloads/company_urls_unique.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea9dcab-88f3-4123-8d63-497a024809a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14d0551-001c-44c1-acfd-1ba5f16126ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(uri)\n",
    "db = client[\"jobscrper\"]\n",
    "collection = db[\"company_metadata\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd36f1-1de9-447b-88c8-7489a6260e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_string(url):\n",
    "    # Find the substring between '//' and the first '/' after it\n",
    "    match = re.search(r'https?://([^./]+)\\.', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d38c27-1b63-4bff-9bb8-2b6c308ef13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    document = {\n",
    "        \"name\": extract_string(row[\"Application-Links\"]),\n",
    "        \"url\": row[\"Application-Links\"]\n",
    "    }\n",
    "    collection.insert_one(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e692ac3-be52-481d-9523-dbb9ed1cb2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = collection.find()\n",
    "test = {\"urls\": []}\n",
    "for document in all_documents:\n",
    "    test[\"urls\"].append(document[\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df676b4-1b1a-40b0-b0f0-ddbc49173b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6222e394-21de-47ad-ae04-f4da9d9201e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229e898d-c12d-4078-9e3b-fbbecff9fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36736f0-ce48-4b5a-a9d7-b914094aa26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = redis.Redis(host='localhost', port=6379, db=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37cb7ae-3cea-4710-bccf-207f23946048",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.set('foo', 'bar')\n",
    "r.get('foo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2d39c4-4e65-4ae1-811a-543ddbb3442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = r.keys('*')\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15f44ff-9829-43e5-96eb-e130826b9c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.delete('foo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2052b8e8-422f-4555-9878-b36fad7a9719",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = r.keys('*')\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d64687-0b5b-4c5f-a566-ada917415741",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.get('foo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4146cdf3-7886-40bb-a8b5-d6778d7b4e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "len('https://genesys.wd1.myworkdayjobs.com/es/Genesys'.split(\"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce09de6-ec9b-4ad3-abd4-3bbcad201dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
